<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Tony Yang</title>

    <meta name="author" content="Tony Yang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Tony Yang
                </p>
                <p>
      I'm a Ph.D. candidate at <a href="https://networks.imdea.org/">IMDEA Networks Institute</a>, where I work with <a href="https://www.joergwidmer.org/">Joerg Widmer</a> conducting research within the <a href="https://dn6sense.eu/"> MSCA 6th Sense Project</a> focusing on designing efficient wireless sensing systems for human/obstacle detection and tracking.
                </p>
                <p>
      Prior, I completed my Master's in Computer and Embedded Systems Engineering at <a href="https://www.tudelft.nl/">TU Delft</a>, where I worked with <a href="https://guohao.netlify.app/">Guohao Lan</a> and <a href="https://www.ccmitss.com/zhang">Xucong Zhang</a> on immersive emotion recognition systems based on eye-tracking technology.
                </p>
                <p>
      I have also worked as a full-time AI research engineer at <a href="https://www.tudelft.nl/tnw/over-faculteit/afdelingen/imphys">ImPhys TU Delft</a>, where I optimized AI models for medical imaging through advanced pruning techniques.
		<!-- I'm a research scientist at <a href="https://deepmind.google/">Google DeepMind</a> in San Francisco, where I lead a small team that mostly works on <a href="https://www.matthewtancik.com/nerf">NeRF</a>.
		At Google I've worked on <a href="https://www.google.com/glass/start/">Glass</a>,  <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://blog.google/products/google-ar-vr/introducing-next-generation-jump/">VR</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, <a href="https://blog.google/products/maps/three-maps-updates-io-2022/">Maps</a>, and <a href="https://research.google/blog/bringing-3d-shoppable-products-online-with-generative-ai/">Shopping</a>.
		I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>.
		I've received the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>. -->
                </p>
                <p style="text-align:center">
                  <a href="mailto:tonyyunyang@outlook.com">Personal Email</a> &nbsp;/&nbsp;
                  <a href="mailto:tongyun.yang@networks.imdea.org">Work Email</a> &nbsp;/&nbsp;
                  <a href="data/tony-cv.pdf">CV</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?hl=en&user=rIFdBYAAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://bsky.app/profile/jonbarron.bsky.social">Bluesky</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/tonyyunyang">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/profile/Profile.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profile/profile_rec.png" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My interests drive by a passion for integrating AI, embedded systems, wearable devices, etc. to create <span class="highlight">impactful, human-centered innovations</span>. Some papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr onmouseout="ubicom25_stop()" onmouseover="ubicom25_start()" bgcolor="#ffffd0">
      <!-- Left column: Image/Video -->
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='ubicom25_image'>
            <!-- For video hover effect -->
            <!-- <video width=100% height=100% muted autoplay loop>
              <source src="images/paper_video.mp4" type="video/mp4">
            </video> -->
            <!-- OR for image hover effect -->
            <img src='images/Ubicomp/2025/IMWUT25.gif' width=100%>
          </div>
          <img src='images/Ubicomp/2025/IMWUT25_1.png' width="160">
        </div>
        <script type="text/javascript">
          function ubicom25_start() {
            document.getElementById('ubicom25_image').style.opacity = "1";
          }
          function ubicom25_stop() {
            document.getElementById('ubicom25_image').style.opacity = "0";
          }
          ubicom25_stop()
        </script>
      </td>
    
      <!-- Right column: Paper details -->
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://github.com/MultiRepEyeVR/Through-the-Eyes-of-Emotion">
          <span class="papertitle">Through the Eyes of Emotion: A Multi-faceted Eye Tracking Dataset
for Emotion Recognition in Virtual Reality</span>
        </a>
        <br>
        <strong>Tongyun Yang</strong>&dagger;,
        Bishwas Regmi&dagger;,
        <a href="https://lingyudu.github.io/">Lingyu Du</a>,
        <a href="https://www.collaborative-ai.org/people/bulling/">Andreas Bulling</a>,
        <a href="https://www.ccmitss.com/zhang">Xucong Zhang</a>,
        <a href="https://guohao.netlify.app/">Guohao Lan</a>
        <br>
        <em>IMWUT/Ubicomp</em>, 2025 &nbsp <font color="red"><strong>(To appear)</strong></font>
        <br>
        <!-- <a href="https://project-page.com">project page</a>
        / -->
        <a href="https://github.com/MultiRepEyeVR/Through-the-Eyes-of-Emotion">arXiv</a>
        /
        <a href="https://github.com/MultiRepEyeVR/Through-the-Eyes-of-Emotion">code</a>
        <p></p>
        <p>
        An eye-tracking dataset in VR, combining high-frame-rate periocular videos and high-frequency gaze data to enable accurate, multimodal emotion recognition.
        </p>
      </td>
    </tr>


    <tr onmouseout="miccai25_stop()" onmouseover="miccai25_start()">
      <!-- Left column: Image/Video -->
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='miccai25_image'>
            <!-- For video hover effect -->
            <!-- <video width=100% height=100% muted autoplay loop>
              <source src="images/paper_video.mp4" type="video/mp4">
            </video> -->
            <!-- OR for image hover effect -->
            <img src='images/MICCAI/2025/Untitled.png' width=100%>
          </div>
          <img src='images/MICCAI/2025/Untitled.png' width="160">
        </div>
        <script type="text/javascript">
          function midl25_start() {
            document.getElementById('miccai25_image').style.opacity = "1";
          }
          function midl25_stop() {
            document.getElementById('miccai25_image').style.opacity = "0";
          }
          midl25_stop()
        </script>
      </td>
    
      <!-- Right column: Paper details -->
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://tonyyunyang.github.io/">
          <span class="papertitle">Reverse Imaging: Any-Sequence Generalization for Cardiac MRI Segmentation</span>
        </a>
        <br>
        Yidong Zhao,
        <a href="https://yizhang025.github.io/">Yi Zhang</a>,
        <strong>Tongyun Yang</strong>,
        Maša Božić-Iven,
        Ayda Arami,
        <a href="https://wexnermedical.osu.edu/find-a-doctor/yuchi-han-md-125529">Yuchi Han</a>,
        <a href="https://medicine.osu.edu/find-faculty/clinical/internal-medicine/orlando-simonetti-phd">Orlando Simonetti</a>,
        <a href="https://www.microsoft.com/en-us/research/people/xueh/">Hui Xue</a>,
        <a href="https://kellmanp.github.io/webpages/curricula_vitae.htm">Petter Kellman</a>,
        <a href="https://www.mars-lab.eu/">Sebastian Weingärtner</a>,
        <a href="https://sites.google.com/view/qtao/">Qian Tao</a>,
        <br>
        <em>MICCAI & IEEE Transactions on Medical Imaging</em>, 2025 <font color="red"><strong>(To appear)</strong></font>
        <br>
        <!-- <a href="https://project-page.com">project page</a>
        / -->
        <a href="https://tonyyunyang.github.io/">arXiv</a>
        <p></p>
        <p>
        A physics-driven framework that estimates tissue properties (M0, T1, T2) from annotated cardiac MRI images using diffusion models, enabling physics-based synthesis of diverse unseen sequences for zero-shot generalization of segmentation models across different MRI contrasts.
        </p>
      </td>
    </tr>


    <tr onmouseout="midl25_stop()" onmouseover="midl25_start()">
      <!-- Left column: Image/Video -->
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='midl25_image'>
            <!-- For video hover effect -->
            <!-- <video width=100% height=100% muted autoplay loop>
              <source src="images/paper_video.mp4" type="video/mp4">
            </video> -->
            <!-- OR for image hover effect -->
            <img src='images/MIDL/2025/MIDL25_1.png' width=100%>
          </div>
          <img src='images/MIDL/2025/MIDL25_1.png' width="160">
        </div>
        <script type="text/javascript">
          function midl25_start() {
            document.getElementById('midl25_image').style.opacity = "1";
          }
          function midl25_stop() {
            document.getElementById('midl25_image').style.opacity = "0";
          }
          midl25_stop()
        </script>
      </td>
    
      <!-- Right column: Paper details -->
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://openreview.net/forum?id=uTTOhthEDR#discussion">
          <span class="papertitle">Pruning nnU-Net with Minimal Performance Loss]{Pruning nnU-Net with Minimal Performance Loss</span>
        </a>
        <br>
        <strong>Tongyun Yang</strong>,
        Yidong Zhao,
        <a href="https://sites.google.com/view/qtao/">Qian Tao</a>,
        <br>
        <em>MIDL</em>, 2025
        <br>
        <!-- <a href="https://project-page.com">project page</a>
        / -->
        <a href="https://openreview.net/forum?id=uTTOhthEDR#discussion">arXiv</a>
        /
        <a href="https://github.com/prunennunet/Prune_nnUNet">code</a>
        <p></p>
        <p>
        Trained nnU-Net models contain substantial weight redundancy, with over 80% of weights removable through simple magnitude-based pruning while maintaining same performance across multiple medical segmentation tasks.
        </p>
      </td>
    </tr>

    <!-- <tr onmouseout="bolt3d_stop()" onmouseover="bolt3d_start()"  bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='bolt3d_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/bolt3d.mp4" type="video/mp4">
          Your browser does not support the video tag.

          </video></div>
          <img src='images/bolt3d.jpg' width="160">
        </div>
        <script type="text/javascript">
          function bolt3d_start() {
            document.getElementById('bolt3d_image').style.opacity = "1";
          }

          function bolt3d_stop() {
            document.getElementById('bolt3d_image').style.opacity = "0";
          }
          bolt3d_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://szymanowiczs.github.io/bolt3d">
          <span class="papertitle">Bolt3D: Generating 3D Scenes in Seconds</span>
        </a>
        <br>
        <a href="https://szymanowiczs.github.io/">Stanislaw Szymanowicz</a>,
        <a href="https://jasonyzhang.com">Jason Y. Zhang</a>,
        <a href="https://pratulsrinivasan.github.io">Pratul Srinivasan</a>,
        <a href="https://ruiqigao.github.io">Ruiqi Gao</a>,
        <a href="https://github.com/ArthurBrussee">Arthur Brussee</a>,
        <a href="https://holynski.org">Aleksander Holynski</a>,
        <a href="https://ricardomartinbrualla.com">Ricardo Martin-Brualla</a>,
		<strong>Jonathan T. Barron</strong>,
        <a href="https://henzler.github.io">Philipp Henzler</a>
        <br>
        <em>ICCV</em>, 2025
        <br>
        <a href="https://szymanowiczs.github.io/bolt3d">project page</a>
        /
        <a href="https://szymanowiczs.github.io/bolt3d">arXiv</a>
        <p></p>
        <p>
		By training a latent diffusion model to directly output 3D Gaussians we enable fast (~6 seconds on a single GPU) feed-forward 3D scene generation.
        </p>
      </td>
    </tr> -->


          </tbody></table>

          
					<table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
           
            <!-- <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #fcb97d;">
								 <h2>Micropapers</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr> -->


            <!-- <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #aaba9e;">
								 <h2>Recorded Talks</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://www.youtube.com/watch?v=hFlF33JZbA0">Radiance Fields and the Future of Generative Media, 2025</a><br>				  
                <a href="https://www.youtube.com/watch?v=h9vq_65eDas">View Dependent Podcast, 2024</a><br>
                <a href="https://www.youtube.com/watch?v=4tDhYsFuEqo">Bay Area Robotics Symposium, 2023
</a><br>
                <a href="https://youtu.be/TvWkwDYtBP4?t=7604">EGSR Keynote, 2021</a><br>
				<a href="https://www.youtube.com/watch?v=nRyOzHpcr4Q">TUM AI Lecture Series, 2020</a><br>
				<a href="https://www.youtube.com/watch?v=HfJpQCBTqZs">Vision & Graphics Seminar at MIT, 2020</a>
              </td>
            </tr> -->

            <!-- <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #c6b89e;">
								 <h2>Academic Service</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://iccv.thecvf.com/">Lead Area Chair, ICCV 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2025/Organizers">Lead Area Chair, CVPR 2025</a>
                <br>
                <a href="https://cvpr.thecvf.com/Conferences/2024/Organizers">Area Chair, CVPR 2024</a>
                <br>
                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
                <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                <br>
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
                <br>
                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                <br>
                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
              </td>
            </tr> -->
						
						
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #edd892;">
								 <h2>Teaching</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://abs-tudelft.github.io/sbd/introduction/index.html">Teaching Assistant, ET 4310 Supercomputing for Big Data (2024/25 Q1)</a>
                <br>
                <a href="https://cese.ewi.tudelft.nl/embedded-systems-lab/">Teaching Assistant, CESE 4030 Embedded Systems Lab (2023/24 Q3)</a>
                <br>
                <a href="https://cese.ewi.tudelft.nl/software-fundamentals/">Teaching Assistant, CESE 4000 Software Fundamentals (2023/24 Q1)</a>
                <br>
                <a href="https://sps.ewi.tudelft.nl/Education/coursedetail.php?mi=127">Teaching Assistant, CESE 4010 Advanced Computing Systems (2023/24 Q1)</a>
                <br>
                <a href="https://www.tudelft.nl/studenten/faculteiten/tbm-studentenportal/onderwijs/bachelor/mentoraat">Graduate Student Mentor (2023/24)</a>
              </td>
            </tr>
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Website adopted from <a href="https://jonbarron.info/">Jon Barron</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
